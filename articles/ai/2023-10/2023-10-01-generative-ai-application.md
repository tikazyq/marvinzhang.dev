# 浅谈生成式 AI 技术：检索增强生成 RAG

## 引言

如今，生成式 AI （Generative AI）的应用雨后春笋的涌现出来，让人应接不暇。而**大型语言模型（LLM，Large Language Model）**，随着 ChatGPT 的发布而变得异常火热，是生成式 AI 应用的一个典型。但是，LLM 存在缺陷。其中一个比较大的问题在于**幻觉（Hallucination）**：对于不熟悉的问题，LLM 会瞎编乱造，编造出看似专业却没有任何事实依据的答案。而为了解决这个问题，很多基于 AI 的知识问答系统采用了**检索增强生成（RAG）**技术，从而让 LLM 能够做出基于事实的回答，从而消除幻觉。本篇文章将简单介绍 RAG 是如何在知识问答系统中发挥作用的。

## LLM

要理解 RAG，我们首先需要简单理解一下 LLM。其实，LLM 在大量的参数训练下，已经可以完成很多难以置信的 NLP 任务，例如问答、写作、翻译、代码理解等等。但是，由于 LLM 的“记忆”停留在预训练时刻，肯定会存在它不知道的知识和问题。例如，OpenAI 开发的 ChatGPT 就不能回答 2021 年 9 月之后的问题。此外，由于幻觉的存在，LLM 会显得很有想象力且缺乏事实依据。因此，我们可以将 LLM 比做一个**知识渊博且全能的智者，可以干很多事情，但却失忆了，记忆只停留在某一时间之前，而且不能形成新的记忆**。

而为了让这个智者能够在现代试题考试中得到高分，我们该怎么做呢？答案就是 RAG。

## RAG

RAG 并不是新技术。早在 2020 年 5 月，LLM 还没有普及的时候，RAG 模型就被提了出来，被用作处理知识密集的 NLP 任务。而如今，RAG 已经成为知识问答、文档检索等基于 AI 技术的应用中非常重要的技术。

回到之前的问题，如果想让 LLM 考试中得到高分，我们该如何做呢？RAG 又是什么原理呢？其实答案很简单，就是我们在 LLM 遇到新问题的时候，我们将教科书中跟问题相关的章节拿出来给 LLM 看，LLM 看到后会根据自己的理解加上相关章节内容结合得出答案。而这个生成的答案会基于事实（教科书），不再是胡说八道。没错，这就跟**开卷考试**一模一样！而这就是 RAG 的原理。

下图是典型知识问答的流程图。当一个问题（Query）来的时候，系统会从知识库（Knowledge Base）中提取相关的上下文（Relevant Context），再将问题和上下文一并喂给 LLM，让 LLM 自行判断并回答该问题，生成响应（Response）。

![RAG原理](https://codao.crawlab.cn/images/2023-10-01-092937.png)

## 嵌入

我们介绍了 LLM 以及 RAG 的简单工作原理，但我们应该如何从知识库中提取相关上下文的呢？这背后的技术就是**嵌入（Embedding）**。嵌入听起来比较专业，但我们也可以比较形象的进行理解。这就好比我们提前将教科书（知识库）的每个章节或每个段落做了**索引（Indexing）**，也就是用各种颜色和记号标记出来。当我们需要回答问题的时候，就从标记好的索引中找出相关的章节段落，再把其抽出来做回答就可以了。是不是很简单？

当然，嵌入技术的本质其实还是将非结构化数据（例如文字）转化为结构化数据（数值矩阵）。而结构化数据正是计算机能够处理和理解的。不光是文字，图片、视频、音频等都可以通过嵌入技术转化为计算机能够理解的结构化数据，而这也是图片搜索引擎的基础原理。

下图是输入文字与输出数值矩阵表示的示意图。

![嵌入技术](https://codao.crawlab.cn/images/2023-10-01-093450.gif)

## 总结

我们通过简单的比喻，将 LLM 比做知识渊博但新记忆缺失的智者，RAG 比做开卷考试，嵌入比做教科书记号，生动形象的阐述了基于生成式 AI 技术的知识问答系统的核心技术。而笔者的智能阅读助手思阅（SRead）也正是基于这样的技术框架，能够让 AI 能够阅读并帮助读者回答与文章、论文、书本相关的问题。希望本文能够让读者对生成式 AI 技术有进一步了解。

![思阅 SRead 界面](https://codao.crawlab.cn/images/2023-10-01-094738.png)

## 社区

如果您对笔者的文章感兴趣，可以加笔者微信 tikazyq1 并注明 "码之道"，笔者会将你拉入 "码之道" 交流群。

智能阅读助手思阅（SRead）上线啦，内测地址: https://sread.ai，欢迎试用。
